---
title: "PracMaLearnAssignment"
author: "Helen Zhang"
date: "Sunday, May 24, 2015"
output: html_document
---

This is the assignment for the cousera class "Practical Machine Learning" from Johns Hokpikns University.
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

```{r}
library(caret)
##extract data from file
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
names(training)
```

As checked from the table above, the first columns are about names and time, which should not be considered as predictors in the final model if we want to ensure the robustness. And the columns with too many "NAs" shall be cleaned as well.

```{r}
##define a function to clean the data.
trim <- function(data){
  for(i in 159:1){
    if(mean(is.numeric(data[,i]))!=1|mean(is.na(data[,i]))!=0){
      data <- data[,-i]
    }
  }
  data
}
##trim training and testing data.
training <- trim(training)
testing <- trim(testing)
```

Then we divide the training set into 3 folds and prepare the data for cross-vaidation.

```{r}
folds <- createFolds(training$classe,k=3,list=TRUE,returnTrain=TRUE)
```

Since the goal is to divide the samples into 5 levels, we first tested on "prediction tree".

```{r}
##try decision tree with 20 principal components.
for(i in 1:3){
  trainSet <- training[folds[[i]],]
  testSet <- training[-folds[[i]],]
  preProc <- preProcess(trainSet[,-57],method="pca",pcaComp=20)
  trainPC <- predict(preProc,trainSet[,-57])
  modFit <- train(trainSet$classe~.,method="rpart",data=trainPC)
  testPC <- predict(preProc,testSet[,-57])
  print(confusionMatrix(testSet$classe,predict(modFit,testPC)))
}
```

And then "random forests".

```{r}
for(i in 1:3){
  trainSet <- training[folds[[i]],]
  testSet <- training[-folds[[i]],]
  preProc <- preProcess(trainSet[,-57],method="pca",pcaComp=20)
  trainPC <- predict(preProc,trainSet[,-57])
  modFit <- train(trainSet$classe~.,method="rf",data=trainPC)
  testPC <- predict(preProc,testSet[,-57])
  print(confusionMatrix(testSet$classe,predict(modFit,testPC)))
}
```

Since the average accuracy of "random forests" is above 95%. I would expect the out of sample error to be less than 5%.

```{r}
##prediction of the testing data.
preProc <- preProcess(training[,-57],method="pca",pcaComp=20)
trainPC <- predict(preProc,training[,-57])
modFit <- train(training$classe~.,method="rf",data=trainPC)
testPC <- predict(preProc,testing[,-57])
predict(modFit,testPC)
```